{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install these required dependencies to run this notebook\n",
    "# !pip install python-dotenv==1.0.0\n",
    "# !pip install requests\n",
    "# !pip install sseclient-py==1.8.0\n",
    "# !pip install pdf2image==1.17.0\n",
    "# !pip install langchain-sambanova\n",
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade0417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "\n",
    "# https://github.com/Unstructured-IO/unstructured  \n",
    "# https://docs.unstructured.io/open-source/core-functionality/chunking\n",
    "# https://github.com/Unstructured-IO/unstructured?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Text, Tables and Images from PDF document\n",
    "# Once we have the PDF downloaded, we will utilize unstructured.io library to process our document and extract the contents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94df86a",
   "metadata": {},
   "source": [
    "# _Extraction of Text and Image summaries_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3700106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pdf_elements=partition_pdf(\n",
    "    filename=\"../ML_PRACTICE/data/NIPS-2017-attention-is-all-you-need-Paper.pdf\",     #put pdf path \n",
    "    strategy=\"hi_res\",\n",
    "    extract_images_in_pdf=True,\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    extract_image_block_to_payload=False,\n",
    "    extract_image_block_output_dir=\"image_store\",\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f96896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pdf_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a061de",
   "metadata": {},
   "source": [
    "### _Identify common elements_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428712c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seen_classes = set()\n",
    "# unique_elements = []\n",
    "\n",
    "# for element in raw_pdf_elements:\n",
    "#     # print(type(element))  # `elements` is your list of parsed elements\n",
    "#     if type(element) not in seen_classes:\n",
    "#         unique_elements.append(element)\n",
    "#         seen_classes.add(type(element))\n",
    "\n",
    "\n",
    "# seen_classes\n",
    "# unique_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32f827",
   "metadata": {},
   "source": [
    "### _store the img,text and table data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58fec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "Header=[]\n",
    "Footer=[]\n",
    "Title=[]\n",
    "NarrativeText=[]\n",
    "Text=[]\n",
    "ListItem=[]\n",
    "\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "  if \"unstructured.documents.elements.Header\" in str(type(element)):\n",
    "            Header.append(str(element))\n",
    "  elif \"unstructured.documents.elements.Footer\" in str(type(element)):\n",
    "            Footer.append(str(element))\n",
    "  elif \"unstructured.documents.elements.Title\" in str(type(element)):\n",
    "            Title.append(str(element))\n",
    "  elif \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
    "            NarrativeText.append(str(element))\n",
    "  elif \"unstructured.documents.elements.Text\" in str(type(element)):\n",
    "            Text.append(str(element))\n",
    "  elif \"unstructured.documents.elements.ListItem\" in str(type(element)):\n",
    "            ListItem.append(str(element))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb844dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=[]\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Image\" in str(type(element)):\n",
    "        img.append(str(element))\n",
    "\n",
    "\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img)):\n",
    "    print(i, img[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table=[]\n",
    "for element in raw_pdf_elements:\n",
    "  if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            table.append(str(element))\n",
    "\n",
    "len(table)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255818d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(table)):\n",
    "    print(i, table[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NarrativeText=[]\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.NarrativeText\" in str(type(element)):\n",
    "            NarrativeText.append(str(element))\n",
    "# NarrativeText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1e41a2",
   "metadata": {},
   "source": [
    "# _Summary of image,text,and table_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2142fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../MULTIMODAL_RAG/model_wrapper.py')  # Add the folder path\n",
    "import model_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_wrapper import SambaNovaCloud\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3849a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "sambanova_api_key = \"SAMBANOVA_API_KEY\"\n",
    "os.environ[\"SAMBANOVA_API_KEY\"] = sambanova_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce23dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_sambanova import ChatSambaNovaCloud\n",
    "\n",
    "llm = ChatSambaNovaCloud(\n",
    "    model=\"Meta-Llama-3.3-70B-Instruct\",\n",
    "    max_tokens=2024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32062a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports ChatPromptTemplate, a LangChain class that lets you define and format prompts for chat-based LLMs\n",
    "\n",
    "\n",
    "#####Chaining\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt_text=\"\"\"You are a helpful assistant tasked with summarizing text.Give a concise summary of the NarrativeText.NarrativeText {element}\"\"\"\n",
    "\n",
    "prompt_table=\"\"\"You are a helpful assistant tasked with summarizing tables .Give a concise summary of the table.Table {element}\"\"\"\n",
    "\n",
    "\n",
    "# These lines convert the plain text strings into LangChain-compatible templates.\n",
    "# You can now use .format_messages(element=\"...\") to inject content into {element}.\n",
    "\n",
    "prompt_text = ChatPromptTemplate.from_template(prompt_text)\n",
    "prompt_table=ChatPromptTemplate.from_template(prompt_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# is creating a data flow pipeline, where each component transforms the input and passes it to the next.\n",
    "#  1. {\"element\": lambda x: x}:The key \"element\" matches the {element} placeholder in your prompt templates.\n",
    "#  2.| prompt_text or | prompt_table:This takes the formatted dictionary and applies it to the prompt using ChatPromptTemplate.\n",
    "#  3. | llm:This sends the prompt to your LLM (Language Model), e.g., sambanova, etc., and gets a response.\n",
    "# 4. | StrOutputParser():This parses the output and converts it into a plain string (instead of an LLM message object).\n",
    "\n",
    "\n",
    "text_summarize_chain = {\"element\": lambda x: x} | prompt_text | llm | StrOutputParser()\n",
    "table_summarize_chain= {\"element\": lambda x: x} | prompt_table | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9270d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(NarrativeText)\n",
    "# .batch() method in LangChain to summarize multiple narrative texts in parallel or controlled batches.\n",
    "NarrativeText_summaries = []\n",
    "if NarrativeText:\n",
    "    NarrativeText_summaries = text_summarize_chain.batch(NarrativeText, {'max_concurrency': 1})  #This sets the maximum number of parallel executions to 1, meaning it processes one input at a time (sequentially)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries = []\n",
    "table_summaries=table_summarize_chain.batch(table,{\"max_concurrency\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc51351",
   "metadata": {},
   "source": [
    "# _Image summary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfdde6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import base64\n",
    "import os \n",
    "from langchain_core.messages import HumanMessage\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    base_url=\"https://api.sambanova.ai/v1\", \n",
    "    api_key=\"SAMBANOVA_API_KEY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_summarizer(prompt,image_base64):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Llama-4-Maverick-17B-128E-Instruct\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            generated_summary = image_summarizer(prompt,base64_image)\n",
    "            print(generated_summary)\n",
    "            image_summaries.append(image_summarizer(prompt,base64_image))\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n",
    "\n",
    "# Image summaries\n",
    "img_base64_list, image_summaries = generate_img_summaries(\"../ML_PRACTICE/image_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d31aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ebf402",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_base64_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c23177",
   "metadata": {},
   "source": [
    "# _Adding to the Vector Store_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "# from langchain_community.embeddings import SambaStudioEmbeddings\n",
    "from langchain_sambanova import SambaNovaCloudEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb7bb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_sambanova import SambaNovaCloudEmbeddings\n",
    "\n",
    "embeddings = SambaNovaCloudEmbeddings(\n",
    "    model=\"E5-Mistral-7B-Instruct\",sambanova_api_key=\"SAMBANOVA_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e816839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a MultiVectorRetriever that indexes embeddings of summaries (text, table, and image) in a vector store, while storing the original content in an in-memory docstore. When queried, the retriever uses vector similarity on the summaries to retrieve the most relevant results and maps them back to the original full documents. This enables fast, semantically accurate retrieval while preserving access to detailed source data.\n",
    "\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(vectorstore,text_summaries, texts, table_summaries, table, image_summaries,img\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a retriever that indexes the summary but returns the original image or text.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "  # Initialize the storage tier\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "        # vectorstore = Chroma(\n",
    "    #     # collection_name='summaries',embedding_function=embeddings )\n",
    "    # print(vectorstore)\n",
    "    # print(table_summaries)\n",
    "    # print(texts)\n",
    "\n",
    "    # The retriever (empty to start)\n",
    "    retriever = MultiVectorRetriever(vectorstore=vectorstore, \n",
    "                                        docstore=store, \n",
    "                                        id_key=id_key)\n",
    "                                        # search_kwargs={'k': 2})  \n",
    "\n",
    "        \n",
    "    print(retriever)\n",
    "\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        print(doc_contents)\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        print(retriever)\n",
    "        summary_docs = [\n",
    "                Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "                for i, s in enumerate(doc_summaries)\n",
    "            ]\n",
    "        print(summary_docs)\n",
    "\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "        # Add texts, tables, and images\n",
    "        # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        # print(text_summaries)\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "       \n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        # print(table_summaries)\n",
    "        add_documents(retriever, table_summaries, table)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, img)\n",
    "      \n",
    "    return retriever\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b2a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(\n",
    "    collection_name='summaries',embedding_function=embeddings,persist_directory=\"sample-rag-multi-modal\" )\n",
    "print(vectorstore)\n",
    "\n",
    "# def create_multi_vector_retriever(vectorstore,text_summaries, texts, table_summaries, table, image_summaries, img\n",
    "# ):\n",
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    NarrativeText_summaries,\n",
    "    NarrativeText,\n",
    "    table_summaries,\n",
    "    table,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    ")\n",
    "\n",
    "retriever_multi_vector_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8adf63",
   "metadata": {},
   "source": [
    "# _RAG_Build_retriever_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\t               Purpose in Pipeline\n",
    "# split_image_text_types:\t   Separate raw data into images vs text\n",
    "# resize_base64_image\t:       Optimize image for LLM input\n",
    "# looks_like_base64\t:       Ensure clean, valid input\n",
    "# img_prompt_func\t   :        Construct a prompt that blends both formats\n",
    "# plt_img_base64\t  :          Optional ‚Äî visualize/debug images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d0399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_base64_images(image_list):\n",
    "    for i, img in enumerate(image_list):\n",
    "        html = f\"<h4>üñºÔ∏è Image {i+1}</h4><img src='data:image/jpeg;base64,{img}' width='500'/>\"\n",
    "        display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5976fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02572b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4e5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282657ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36f307",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(400, 300))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505936d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are analyst and advice.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            \"Use this information to provide answer related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6010c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_sambanova import ChatSambaNovaCloud\n",
    "\n",
    "llm_1= ChatSambaNovaCloud(\n",
    "    model=\"Llama-4-Maverick-17B-128E-Instruct\",\n",
    "    # max_tokens=2024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dadb840",
   "metadata": {},
   "source": [
    "##### ***_multi_modal_rag_chain(...) builds the entire chain:_\n",
    "\n",
    "#### _Retrieves relevant text and images_\n",
    "\n",
    "#### _Splits them using split_image_text_types_\n",
    "\n",
    "#### _Formats a multimodal prompt with img_prompt_func_\n",
    "\n",
    "#### _Passes it to the LLM (Llama-4-Maverick-17B-128E-Instruct)_\n",
    "\n",
    "#### _Parses the result into a plain string_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acfc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatSambaNovaCloud(model=\"Llama-4-Maverick-17B-128E-Instruct\",temperature=0.7,top_p=0.01)\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da3675e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_multimodal_rag(query_text):\n",
    "    # Run the multimodal RAG chain\n",
    "    response = chain_multimodal_rag.invoke(query_text)\n",
    "    \n",
    "    # Print response (LLM's answer)\n",
    "    print(\"Answer:\\n\", response)\n",
    "    \n",
    "    # Get docs separately (already retrieved)\n",
    "    docs = retriever_multi_vector_img.invoke(query_text, limit= 6)\n",
    "    \n",
    "    # Display all images retrieved and detected as base64 images\n",
    "    for img in split_image_text_types(docs)[\"images\"]:\n",
    "        plt_img_base64(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8076ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User can enter the query\n",
    "query_text = input(\"Enter your query: \")\n",
    "query_multimodal_rag(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671f064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)\n",
    "query_text = \"What is transformer?\"\n",
    "\n",
    "# Run the multimodal RAG chain\n",
    "response = chain_multimodal_rag.invoke(query_text)\n",
    "\n",
    "# Print response (LLM's answer)\n",
    "print(\"Answer:\\n\", response)\n",
    "\n",
    "# Get docs separately (already retrieved)\n",
    "docs = retriever_multi_vector_img.invoke(query_text, limit=6)\n",
    "\n",
    "# Display all images retrieved and detected as base64 images\n",
    "for img in split_image_text_types(docs)[\"images\"]:\n",
    "    plt_img_base64(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8146951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Query by Image ---\n",
    "\n",
    "\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from IPython.display import display, Image as IPyImage\n",
    "import re\n",
    "\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)\n",
    "query_text = \"What is mult-head-attention?\"\n",
    "\n",
    "docs = retriever_multi_vector_img.invoke(query_text, limit=6)\n",
    "\n",
    "def is_base64_image(text):\n",
    "    return isinstance(text, str) and re.match(r'^/9j|^iVB', text.strip())  # JPEG or PNG signatures\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"{i}:\", end=\" \")\n",
    "\n",
    "    if is_base64_image(doc):\n",
    "        try:\n",
    "            # Display inline image\n",
    "            image_data = base64.b64decode(doc)\n",
    "            display(IPyImage(data=image_data))\n",
    "        except Exception as e:\n",
    "            print(f\"[Error displaying image]: {e}\")\n",
    "    else:\n",
    "        print(doc)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d29ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Query by TEXT ---\n",
    "\n",
    "query_text = \"What is mult-head-attention?\"\n",
    "\n",
    "response_text = chain_multimodal_rag.invoke(query_text)\n",
    "print(\"Response to text query:\")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83528c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6cb1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db61f6f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d2b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ab431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c5b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
